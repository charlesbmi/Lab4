EE 108B Cache Design - LAB4

Charles Guan
Nipun Agarwala

12th March, 2014

INTRODUCTION:
In the given lab, we modified the cache.v, dataram.v and irom.v to add the functionality of caching to the memory stage
of the single cycled processor. The input to the memory checks whether the address being asked for is in the cache. If it is, the memory stage responds by giving the data in the same clock cycle. Otherwise, it stalls the processor and fetches the data from the dataram. The irom.v module contains instructions to check whether the cache.v module works fine.


DESIGN:
We first extract out the required data from the address passed in to find the offset, index and the tag of the data input.
The we check whether the instruction is either a read, write or neither. If it is a read, we check whether the cache has
a copy of the address. If som, we return the data within the same clock cycle. Otherwise, we check the dataram for the address and get an entire block of data from which we output the required data and also update the cache after the required latency. This is when the dram complete signal goes high. Simultaneously, we set the valid bits and the tag appropriately. 
If it is a write signal, then we write through to the dataram. We check whether the data is present in the cache too. If it is present, then we overwrite the data in the cache with the incoming data at the required tag and offset.


RESULTS:
A direct-mapped, write-through, no-write allocate data cache was implemented for a single-cycle processor. The cache contained 64 blocks of 4 words each, for a 1 KB cache storage. This was the most straightforward cache style but still provides memory access improvements. Deciding on a replacement policy was unnecessary because the cache was direct-mapped and would replace automatically. Additionally, writing data works similarly even if data is in the cache due to its write-through nature. However, the simplicity also sacrifices performance. However, since the program used, as well as the implemented pong had frequent spatial locality, the cache performed well. The cache was tested for accuracy on read misses, read hits, write misses, write hits, and write hits to adjacent blocks.
The initial design had issues with consecutive writing to the cache using din because of blocking assignment updating at the end of cycles and causing timing issues. These were solved by using non-blocking statements and understanding the correct dependencies of always blocks. One issue that we quickly resolved was the issue of when to assign valid and tag bits in the cache on read misses. The initial intention was to set the bits as soon as the memory access had started. However, this caused a 1-cycle exit from the memory access, because the cache would then find a read hit immediately after. The solution was to set the tag bit during a read enable and a complete sign from the memory. This would effectively overwrite the cache tag on read hits, but since the address was the same, this does not cause any issues.

CONCLUSIONS:
A direct-mapped cache implementation was used to demonstrate latency improvements over a single large memory access sytem. The implementation utilized fundamental addressing and replacement patterns. Improvements in performance could be added by using set-associative caching or different designs at the cost of extra wires and increased complexity. The design was good in that it abstracted the read and write cases from the above and lower layers. However, some of the ways wires and registers could have been assigned more cleanly, and we weren't sure how to get around the dependencies in our read always block without creating further bugs. Overall, the caching lab went well.

ADDITIONAL QUESTIONS:
1. Our cache will perform best on frequent reads to the same block or blocks in memory. This pattern will allow the cache to take advantage of spatial and temporal locality. Since our system is write-through, all write operations will be limited by memory latency and will thus be slow.
2. Our implementation of Pong would have passable cache performance. Excluding display and controller reads and writes, most reads are stack returns, meaning they also require a slow write instruction prior to it. The implementation would have better cache performance if there were many more reads than writes. However, since the stack pointer does not move a lot, the cache takes advantage of spatial locality.
